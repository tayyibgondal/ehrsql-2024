{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Original Implementation by Gyubok Lee -->\n",
        "<!-- Refined by Sunjun Kweon on 2024-01-15. -->\n",
        "<!-- Note: This Jupyter notebook is tailored to the unique requirements of the EHRSQL project. It includes specific modifications and additional adjustments to cater to the dataset and experiment objectives. -->\n",
        "\n",
        "# OpenAI Model (ChatGPT) Sample Code for EHRSQL: Reliable Text-to-SQL Modeling on Electronic Health Records\n",
        "\n",
        "<p align=\"left\" float=\"left\">\n",
        "  <img src=\"https://github.com/glee4810/ehrsql-2024/raw/master/image/logo.png\" height=\"100\" />\n",
        "</p>\n",
        "\n",
        "<!-- ## Task Introduction\n",
        "The goal of the task is to **develop a reliable text-to-SQL system on EHR**. Unlike standard text-to-SQL tasks, this system must handle all types of questions, including answerable and unanswerable ones with respect to the EHR database structure. For answerable questions, the system must accurately generate SQL queries. For unanswerable questions, the system must correctly identify them as such, thereby preventing incorrect SQL predictions for infeasible questions. The range of questions includes answerable queries about MIMIC-IV, covering topics such as patient demographics, vital signs, and specific disease survival rates ([EHRSQL](https://github.com/glee4810/EHRSQL)). Additionally, there are specially designed unanswerable questions intended to challenge the system. Successfully completing this task will result in the creation of a reliable question-answering system for EHRs, significantly improving the flexibility and efficiency of clinical knowledge exploration in hospitals. -->\n",
        "\n",
        "## Steps of Baseline Code\n",
        "\n",
        "- [x] Step 0: Prerequisites (OpenAI API)\n",
        "- [x] Step 1: Clone the GitHub Repository and Install Dependencies\n",
        "- [x] Step 2: Import Global Packages and Define File Paths\n",
        "- [x] Step 3: Load Data and Prepare Datasets\n",
        "- [x] Step 4: Building a predictive model using chatGPT\n",
        "- [x] Step 5: Submission\n"
      ],
      "metadata": {
        "id": "fV7pV4ue_8Ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 : Prerequisites (OpenAI API key)\n",
        "\n"
      ],
      "metadata": {
        "id": "kw9GLCf8EKb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "from IPython.display import clear_output\n",
        "\n",
        "clear_output()\n",
        "# Please enter your API key\n",
        "new_api_key = ''\n",
        "while len(new_api_key) == 0:\n",
        "    new_api_key = getpass.getpass(\"Please input your API key: \")\n",
        "    clear_output()"
      ],
      "metadata": {
        "id": "_OqDSH_2ES8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When submitting your code for code verification, please be sure to submit your OpenAI API key along with your code, like in the sample_submission_chatgpt_api_key.json."
      ],
      "metadata": {
        "id": "eh80GM31Yk-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Clone the GitHub Repository and Install Dependencies"
      ],
      "metadata": {
        "id": "76ZXSJhhAqZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you begin, make sure you're in the correct directory. If you need to reset the repository directory, remove the existing directory by uncommenting and executing the following lines:"
      ],
      "metadata": {
        "id": "S1hbegr4BADN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf ehrsql-2024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc_VUmzR_83L",
        "outputId": "7a3fa276-e511-4af2-d96d-ab5306edfa06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, clone the repository and install the required Python packages:"
      ],
      "metadata": {
        "id": "OBpg7kpMBEHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning the GitHub repository\n",
        "!git clone -q https://github.com/glee4810/ehrsql-2024.git\n",
        "%cd ehrsql-2024\n",
        "\n",
        "# Installing dependencies\n",
        "!pip install -q tiktoken openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLOhRzcCBB_d",
        "outputId": "cea6f663-997d-4b5a-e013-756ad66de458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehrsql-2024\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `%load_ext` magic command to automatically reload modules before executing a new line:"
      ],
      "metadata": {
        "id": "O5D2VbclBcJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "1ocUNohyBHUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Global Packages and Define File Paths\n",
        "\n",
        "After setting up the repository and dependencies, the next step is to import packages that will be used globally throughout this notebook and to define the file paths to our datasets."
      ],
      "metadata": {
        "id": "wr2zM1EVBj-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directory paths for database, results and scoring program\n",
        "DB_ID = 'mimic_iv'\n",
        "BASE_DATA_DIR = 'sample_data'\n",
        "RESULT_DIR = 'sample_result_submission/'\n",
        "SCORE_PROGRAM_DIR = 'scoring_program/'\n",
        "\n",
        "# File paths for the dataset and labels\n",
        "TABLES_PATH = os.path.join('data', DB_ID, 'tables.json')               # JSON containing database schema\n",
        "TRAIN_DATA_PATH = os.path.join(BASE_DATA_DIR, 'train', 'data.json')    # JSON file with natural language questions for training data\n",
        "TRAIN_LABEL_PATH = os.path.join(BASE_DATA_DIR, 'train', 'label.json')  # JSON file with corresponding SQL queries for training data\n",
        "VALID_DATA_PATH = os.path.join(BASE_DATA_DIR, 'valid', 'data.json')    # JSON file for validation data\n",
        "DB_PATH = os.path.join('data', DB_ID, f'{DB_ID}.sqlite')               # Database path"
      ],
      "metadata": {
        "id": "PuvpWuLRBeo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function loads and processes a database schema from a JSON file.\n",
        "\n",
        "def load_schema(DATASET_JSON):\n",
        "    schema_df = pd.read_json(DATASET_JSON)\n",
        "    schema_df = schema_df.drop(['column_names','table_names'], axis=1)\n",
        "    schema = []\n",
        "    f_keys = []\n",
        "    p_keys = []\n",
        "    for index, row in schema_df.iterrows():\n",
        "        tables = row['table_names_original']\n",
        "        col_names = row['column_names_original']\n",
        "        col_types = row['column_types']\n",
        "        foreign_keys = row['foreign_keys']\n",
        "        primary_keys = row['primary_keys']\n",
        "        for col, col_type in zip(col_names, col_types):\n",
        "            index, col_name = col\n",
        "            if index > -1:\n",
        "                schema.append([row['db_id'], tables[index], col_name, col_type])\n",
        "        for primary_key in primary_keys:\n",
        "            index, column = col_names[primary_key]\n",
        "            p_keys.append([row['db_id'], tables[index], column])\n",
        "        for foreign_key in foreign_keys:\n",
        "            first, second = foreign_key\n",
        "            first_index, first_column = col_names[first]\n",
        "            second_index, second_column = col_names[second]\n",
        "            f_keys.append([row['db_id'], tables[first_index], tables[second_index], first_column, second_column])\n",
        "    db_schema = pd.DataFrame(schema, columns=['Database name', 'Table Name', 'Field Name', 'Type'])\n",
        "    primary_key = pd.DataFrame(p_keys, columns=['Database name', 'Table Name', 'Primary Key'])\n",
        "    foreign_key = pd.DataFrame(f_keys,\n",
        "                        columns=['Database name', 'First Table Name', 'Second Table Name', 'First Table Foreign Key',\n",
        "                                 'Second Table Foreign Key'])\n",
        "    return db_schema, primary_key, foreign_key\n",
        "\n",
        "# Generates a string representation of foreign key relationships in a MySQL-like format for a specific database.\n",
        "def find_foreign_keys_MYSQL_like(foreign, db_id):\n",
        "    df = foreign[foreign['Database name'] == db_id]\n",
        "    output = \"[\"\n",
        "    for index, row in df.iterrows():\n",
        "        output += row['First Table Name'] + '.' + row['First Table Foreign Key'] + \" = \" + row['Second Table Name'] + '.' + row['Second Table Foreign Key'] + ', '\n",
        "    output = output[:-2] + \"]\"\n",
        "    if len(output)==1:\n",
        "        output = '[]'\n",
        "    return output\n",
        "\n",
        "# Creates a string representation of the fields (columns) in each table of a specific database, formatted in a MySQL-like syntax.\n",
        "def find_fields_MYSQL_like(db_schema, db_id):\n",
        "    df = db_schema[db_schema['Database name'] == db_id]\n",
        "    df = df.groupby('Table Name')\n",
        "    output = \"\"\n",
        "    for name, group in df:\n",
        "        output += \"Table \" +name+ ', columns = ['\n",
        "        for index, row in group.iterrows():\n",
        "            output += row[\"Field Name\"]+', '\n",
        "        output = output[:-2]\n",
        "        output += \"]\\n\"\n",
        "    return output\n",
        "\n",
        "# Generates a comprehensive textual prompt describing the database schema, including tables, columns, and foreign key relationships.\n",
        "def create_schema_prompt(db_id, db_schema, primary_key, foreign_key, is_lower=True):\n",
        "    prompt = find_fields_MYSQL_like(db_schema, db_id)\n",
        "    prompt += \"Foreign_keys = \" + find_foreign_keys_MYSQL_like(foreign_key, db_id)\n",
        "    if is_lower:\n",
        "        prompt = prompt.lower()\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "LrNlWHgUQ4pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Data and Prepare Datasets\n",
        "\n",
        "Now that we have our environment and paths set up, the next step is to load the data and prepare it for our model. This involves preprocessing the MIMIC-IV database, reading the data from JSON files, splitting it into training and validation sets, and then initializing our dataset object."
      ],
      "metadata": {
        "id": "grmmBy9cOwg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.data_io import read_json as read_data\n",
        "\n",
        "db_schema, primary_key, foreign_key = load_schema(TABLES_PATH)\n",
        "\n",
        "train_data = read_data(TRAIN_DATA_PATH)\n",
        "train_label = read_data(TRAIN_LABEL_PATH)\n",
        "\n",
        "valid_data = read_data(VALID_DATA_PATH)\n",
        "\n",
        "table_prompt = create_schema_prompt(DB_ID, db_schema, primary_key, foreign_key)"
      ],
      "metadata": {
        "id": "m7rIlLiiOxXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Statistics"
      ],
      "metadata": {
        "id": "HUUETad0PtGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train data:\", (len(train_data['data']), len(train_label)))\n",
        "print(\"Valid data:\", len(valid_data['data']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2x-VlDWPrLZ",
        "outputId": "8ac3519b-6b74-44dc-aca9-d50058d6b6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: (20, 20)\n",
            "Valid data: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Format\n",
        "\n",
        "Before proceeding with the model, it is always a good idea to explore the dataset. This includes checking the keys in the dataset, and viewing the first few entries to understand the structure of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_MRV2nY_P1C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore keys and data structure\n",
        "print(train_data.keys())\n",
        "print(train_data['version'])\n",
        "print(train_data['data'][0])\n",
        "\n",
        "# Explore the label structure\n",
        "print(train_label.keys())\n",
        "print(train_label[list(train_label.keys())[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjG8u-kwP6kB",
        "outputId": "5121b6ed-8dae-438f-e19c-855e7b9278e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['version', 'data'])\n",
            "mimiciii_v1.0_sample\n",
            "{'question': 'had there been a microbiology test done until 35 months ago for patient 46330?', 'id': '2680598b7213c3a13024d289'}\n",
            "dict_keys(['2680598b7213c3a13024d289', 'bf0911c0b58077c8f1d6f81c', '63878496a698752a63ebef4c', 'd274aaeaebb3bf290111c9e9', 'fc1a576ff1dca02da376ab1f', 'e246f17f4e96c69fa0e1ca12', 'b43caaea4224eabfa85a17c6', '242cd957777ec217877ec636', 'ca1485115687adc483443bf0', 'a957874c4a875205a8924e22', '5123baaf7143d5e7e673608f', 'ba96ef29e76cefcafd105d48', '6134d95b33e1f45d8c4cb3ae', '6d0e8046e4803f7cdc34bc07', '623cb0c0669e682042e8db7f', '4ebfb78b77f705129db7d86b', '0110ce1384fc2de34f7201ea', 'a742ea513b62cac50e64948d', '349c9a170043e0a7eca2ba7e', 'dd1b63bf2b32ea3d905a813b'])\n",
            "select count(*)>0 from microbiologyevents where microbiologyevents.hadm_id in ( select admissions.hadm_id from admissions where admissions.subject_id = 46330 ) and datetime(microbiologyevents.charttime) <= datetime(current_time,'-35 month')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for chatGPT containing meta-data of the database such as columns and foreign keys\n",
        "\n",
        "print(table_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVdZyFYwSSth",
        "outputId": "12fa3b9c-0d76-4709-ae32-32f8d9d137bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "table admissions, columns = [row_id, subject_id, hadm_id, admittime, dischtime, admission_type, admission_location, discharge_location, insurance, language, marital_status, age]\n",
            "table chartevents, columns = [row_id, subject_id, hadm_id, stay_id, itemid, charttime, valuenum, valueuom]\n",
            "table cost, columns = [row_id, subject_id, hadm_id, event_type, event_id, chargetime, cost]\n",
            "table d_icd_diagnoses, columns = [row_id, icd_code, long_title]\n",
            "table d_icd_procedures, columns = [row_id, icd_code, long_title]\n",
            "table d_items, columns = [row_id, itemid, label, abbreviation, linksto]\n",
            "table d_labitems, columns = [row_id, itemid, label]\n",
            "table diagnoses_icd, columns = [row_id, subject_id, hadm_id, icd_code, charttime]\n",
            "table icustays, columns = [row_id, subject_id, hadm_id, stay_id, first_careunit, last_careunit, intime, outtime]\n",
            "table inputevents, columns = [row_id, subject_id, hadm_id, stay_id, starttime, itemid, amount]\n",
            "table labevents, columns = [row_id, subject_id, hadm_id, itemid, charttime, valuenum, valueuom]\n",
            "table microbiologyevents, columns = [row_id, subject_id, hadm_id, charttime, spec_type_desc, test_name, org_name]\n",
            "table outputevents, columns = [row_id, subject_id, hadm_id, stay_id, charttime, itemid, value]\n",
            "table patients, columns = [row_id, subject_id, gender, dob, dod]\n",
            "table prescriptions, columns = [row_id, subject_id, hadm_id, starttime, stoptime, drug, dose_val_rx, dose_unit_rx, route]\n",
            "table procedures_icd, columns = [row_id, subject_id, hadm_id, icd_code, charttime]\n",
            "table transfers, columns = [row_id, subject_id, hadm_id, transfer_id, eventtype, careunit, intime, outtime]\n",
            "foreign_keys = [admissions.subject_id = patients.subject_id, diagnoses_icd.hadm_id = admissions.hadm_id, diagnoses_icd.icd_code = d_icd_diagnoses.icd_code, procedures_icd.hadm_id = admissions.hadm_id, procedures_icd.icd_code = d_icd_procedures.icd_code, labevents.hadm_id = admissions.hadm_id, labevents.itemid = d_labitems.itemid, prescriptions.hadm_id = admissions.hadm_id, cost.hadm_id = admissions.hadm_id, cost.event_id = diagnoses_icd.row_id, cost.event_id = procedures_icd.row_id, cost.event_id = labevents.row_id, cost.event_id = prescriptions.row_id, chartevents.hadm_id = admissions.hadm_id, chartevents.stay_id = icustays.stay_id, chartevents.itemid = d_items.itemid, inputevents.hadm_id = admissions.hadm_id, inputevents.stay_id = icustays.stay_id, inputevents.itemid = d_items.itemid, outputevents.hadm_id = admissions.hadm_id, outputevents.stay_id = icustays.stay_id, outputevents.itemid = d_items.itemid, microbiologyevents.hadm_id = admissions.hadm_id, icustays.hadm_id = admissions.hadm_id, transfers.hadm_id = admissions.hadm_id]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Building a predictive model using chatGPT"
      ],
      "metadata": {
        "id": "uFY6RgyFSVgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your api key into json file\n",
        "import json\n",
        "\n",
        "api_path = 'sample_submission_chatgpt_api_key.json'\n",
        "json_data = {}\n",
        "json_data['key'] = new_api_key\n",
        "with open(api_path, 'w') as file:\n",
        "    json.dump(json_data, file, indent=4)"
      ],
      "metadata": {
        "id": "nhtcBmB-iuPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import tiktoken\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=json_data['key'])\n",
        "\n",
        "def post_process(answer):\n",
        "    answer = answer.replace('\\n', ' ')\n",
        "    answer = re.sub('[ ]+', ' ', answer)\n",
        "    return answer\n",
        "\n",
        "class Model():\n",
        "    def __init__(self):\n",
        "        current_real_dir = os.getcwd()\n",
        "        # current_real_dir = os.path.dirname(os.path.realpath(__file__))\n",
        "        target_dir = os.path.join(current_real_dir, 'sample_submission_chatgpt_api_key.json')\n",
        "\n",
        "        if os.path.isfile(target_dir):\n",
        "            with open(target_dir, 'rb') as f:\n",
        "                openai.api_key = json.load(f)['key']\n",
        "        if not os.path.isfile(target_dir) or openai.api_key == \"\":\n",
        "            raise Exception(\"Error: no API key file found.\")\n",
        "\n",
        "    def ask_chatgpt(self, prompt, model=\"gpt-3.5-turbo-16k\", temperature=0.0):\n",
        "        response = client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    temperature=temperature,\n",
        "                    messages=prompt\n",
        "                )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def generate(self, input_data):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            input_data: list of python dictionaries containing 'id' and 'input'\n",
        "        Returns:\n",
        "            labels: python dictionary containing sql prediction or 'null' values associated with ids\n",
        "        \"\"\"\n",
        "\n",
        "        labels = {}\n",
        "\n",
        "        for sample in input_data:\n",
        "            answer = self.ask_chatgpt(sample['input'])\n",
        "            labels[sample[\"id\"]] = post_process(answer)\n",
        "\n",
        "        return labels"
      ],
      "metadata": {
        "id": "oYtBKk5iSXmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "47d31e61-d2b1-4723-d5e2-4eb4903a863c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0e7d15964d70>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myModel = Model()\n",
        "data = valid_data[\"data\"]"
      ],
      "metadata": {
        "id": "1KF30SxDgTOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt for chatGPT\n",
        "system_msg = \"Given the following SQL tables, your job is to write queries given a user’s request. If you think you cannot get the correct SQL, answer with 'null'.\""
      ],
      "metadata": {
        "id": "w7SYpvvigVw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = []\n",
        "for sample in tqdm(data):\n",
        "    sample_dict = {}\n",
        "    sample_dict['id'] = sample['id']\n",
        "    conversation = [{\"role\": \"system\", \"content\": system_msg + '\\n\\n' + table_prompt}]\n",
        "    user_question_wrapper = lambda question: '\\n\\n' + f\"\"\"NLQ: \\\"{question}\\\"\\nSQL: \\\"\"\"\"\n",
        "    conversation.append({\"role\": \"user\", \"content\": user_question_wrapper(sample['question'])})\n",
        "    sample_dict['input'] = conversation\n",
        "    input_data.append(sample_dict)"
      ],
      "metadata": {
        "id": "yEHYS67WGt_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cb4319-0dd7-498d-e6e4-51f35ca26af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 25274.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First message\n",
        "\n",
        "print(conversation[0]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ixE5GuRgX0T",
        "outputId": "0ca4b82e-d551-4223-f5d6-65c5440642aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given the following SQL tables, your job is to write queries given a user’s request. If you think you cannot get the correct SQL, answer with 'null'.\n",
            "\n",
            "table admissions, columns = [row_id, subject_id, hadm_id, admittime, dischtime, admission_type, admission_location, discharge_location, insurance, language, marital_status, age]\n",
            "table chartevents, columns = [row_id, subject_id, hadm_id, stay_id, itemid, charttime, valuenum, valueuom]\n",
            "table cost, columns = [row_id, subject_id, hadm_id, event_type, event_id, chargetime, cost]\n",
            "table d_icd_diagnoses, columns = [row_id, icd_code, long_title]\n",
            "table d_icd_procedures, columns = [row_id, icd_code, long_title]\n",
            "table d_items, columns = [row_id, itemid, label, abbreviation, linksto]\n",
            "table d_labitems, columns = [row_id, itemid, label]\n",
            "table diagnoses_icd, columns = [row_id, subject_id, hadm_id, icd_code, charttime]\n",
            "table icustays, columns = [row_id, subject_id, hadm_id, stay_id, first_careunit, last_careunit, intime, outtime]\n",
            "table inputevents, columns = [row_id, subject_id, hadm_id, stay_id, starttime, itemid, amount]\n",
            "table labevents, columns = [row_id, subject_id, hadm_id, itemid, charttime, valuenum, valueuom]\n",
            "table microbiologyevents, columns = [row_id, subject_id, hadm_id, charttime, spec_type_desc, test_name, org_name]\n",
            "table outputevents, columns = [row_id, subject_id, hadm_id, stay_id, charttime, itemid, value]\n",
            "table patients, columns = [row_id, subject_id, gender, dob, dod]\n",
            "table prescriptions, columns = [row_id, subject_id, hadm_id, starttime, stoptime, drug, dose_val_rx, dose_unit_rx, route]\n",
            "table procedures_icd, columns = [row_id, subject_id, hadm_id, icd_code, charttime]\n",
            "table transfers, columns = [row_id, subject_id, hadm_id, transfer_id, eventtype, careunit, intime, outtime]\n",
            "foreign_keys = [admissions.subject_id = patients.subject_id, diagnoses_icd.hadm_id = admissions.hadm_id, diagnoses_icd.icd_code = d_icd_diagnoses.icd_code, procedures_icd.hadm_id = admissions.hadm_id, procedures_icd.icd_code = d_icd_procedures.icd_code, labevents.hadm_id = admissions.hadm_id, labevents.itemid = d_labitems.itemid, prescriptions.hadm_id = admissions.hadm_id, cost.hadm_id = admissions.hadm_id, cost.event_id = diagnoses_icd.row_id, cost.event_id = procedures_icd.row_id, cost.event_id = labevents.row_id, cost.event_id = prescriptions.row_id, chartevents.hadm_id = admissions.hadm_id, chartevents.stay_id = icustays.stay_id, chartevents.itemid = d_items.itemid, inputevents.hadm_id = admissions.hadm_id, inputevents.stay_id = icustays.stay_id, inputevents.itemid = d_items.itemid, outputevents.hadm_id = admissions.hadm_id, outputevents.stay_id = icustays.stay_id, outputevents.itemid = d_items.itemid, microbiologyevents.hadm_id = admissions.hadm_id, icustays.hadm_id = admissions.hadm_id, transfers.hadm_id = admissions.hadm_id]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second message\n",
        "\n",
        "print(conversation[1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD0CRSiUgZJd",
        "outputId": "2848aa0c-8d9d-4782-da88-e873c5579af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "NLQ: \"what are the precautions to take after the packed cell transfusion procedure?\"\n",
            "SQL: \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer(SQL) from chatGPT\n",
        "label_y = myModel.generate(input_data)"
      ],
      "metadata": {
        "id": "i7VkY7y1gaW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is how the predicted labels(SQLs) look like"
      ],
      "metadata": {
        "id": "Tvice6Gsu-hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDiQKx4Wgb8C",
        "outputId": "de026f4d-2637-4fb3-e445-851ba0e8676d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d93058c12f048d80eaae1c99': 'SELECT gender FROM patients WHERE subject_id = 70709\"',\n",
              " 'd3941cc75cbecd1ca1eedc6e': 'SELECT p.drug FROM prescriptions p JOIN admissions a ON p.hadm_id = a.hadm_id JOIN procedures_icd pi ON a.hadm_id = pi.hadm_id WHERE p.subject_id = 14467 AND pi.icd9_code = \\'oth extraoc mus-tend op\\' AND p.startdate <= DATE_ADD(pi.charttime, INTERVAL 2 DAY) AND p.enddate >= pi.charttime\"',\n",
              " 'dc1c58baffd2987e2ad7b582': 'SELECT d_icd_procedures.short_title, COUNT(*) AS count FROM procedures_icd JOIN admissions ON procedures_icd.hadm_id = admissions.hadm_id JOIN d_icd_procedures ON procedures_icd.icd9_code = d_icd_procedures.icd9_code WHERE admissions.age >= 20 AND admissions.age < 30 AND EXTRACT(YEAR FROM admissions.admittime) = EXTRACT(YEAR FROM CURRENT_DATE) GROUP BY d_icd_procedures.short_title ORDER BY count DESC LIMIT 4;\"',\n",
              " '5ff51cd11c92b00c656f621d': 'SELECT COUNT(DISTINCT admissions.subject_id) FROM admissions JOIN diagnoses_icd ON admissions.hadm_id = diagnoses_icd.hadm_id JOIN d_icd_diagnoses ON diagnoses_icd.icd9_code = d_icd_diagnoses.icd9_code WHERE d_icd_diagnoses.long_title LIKE \\'%ch ob asthma w stat asth%\\' AND admissions.dischtime >= diagnoses_icd.charttime AND admissions.dischtime <= DATEADD(month, 1, diagnoses_icd.charttime) AND admissions.dod IS NOT NULL\"',\n",
              " 'f529165cd5190762b1bb8da2': 'SELECT d_labitems.label FROM labevents JOIN d_labitems ON labevents.itemid = d_labitems.itemid WHERE labevents.subject_id = 8814 AND labevents.hadm_id = (SELECT hadm_id FROM admissions WHERE subject_id = 8814 ORDER BY admittime ASC LIMIT 1) ORDER BY labevents.charttime DESC LIMIT 1\"',\n",
              " 'e2689b46a1bb0237aac9ccc2': 'SELECT drug FROM prescriptions WHERE subject_id = 27739 AND startdate <= \\'2013-11-30\\' AND enddate >= \\'2013-11-01\\' ORDER BY enddate DESC LIMIT 1\"',\n",
              " '209786c44329bb2f8050b0cd': \"SELECT spec_type_desc FROM microbiologyevents WHERE subject_id = 70267 AND charttime >= '2021-12-01' AND charttime < '2022-01-01' ORDER BY charttime LIMIT 1;\",\n",
              " 'e395200ae4f43d00013f61d4': 'SELECT COUNT(DISTINCT subject_id) AS num_patients FROM admissions WHERE discharge_location IS NOT NULL AND dischtime >= DATE_SUB(CURDATE(), INTERVAL 1 YEAR)',\n",
              " 'ed7856b70a4685aaeba70ccf': \"SELECT MIN(valuenum) AS min_heart_rate, MIN(charttime) AS first_min_heart_rate FROM chartevents WHERE subject_id = 2238 AND itemid = (SELECT itemid FROM d_items WHERE label = 'Heart Rate') GROUP BY subject_id ORDER BY first_min_heart_rate ASC LIMIT 1;\",\n",
              " '4aba351edb1637b6556e2adc': 'SELECT drug, COUNT(*) as prescription_count FROM prescriptions WHERE drug IN ( SELECT drug FROM prescriptions WHERE drug = \\'morphine sr (ms contin)\\' AND YEAR(startdate) = 2015 ) AND YEAR(startdate) = 2015 GROUP BY drug ORDER BY prescription_count DESC LIMIT 3\"',\n",
              " '91201a403db0ec0cdbfbe5a9': 'SELECT d_labitems.label FROM labevents JOIN d_labitems ON labevents.itemid = d_labitems.itemid WHERE labevents.subject_id = 54281 AND labevents.hadm_id <> (SELECT hadm_id FROM admissions WHERE subject_id = 54281) AND labevents.hadm_id IN (SELECT hadm_id FROM admissions WHERE subject_id = 54281)',\n",
              " '5cd214580d27adebfaac2ea3': 'SELECT admission_location FROM admissions WHERE subject_id = 97734\"',\n",
              " 'b17fd151d64dbd7ffb8d0663': 'SELECT COUNT(*) FROM labevents WHERE hadm_id = 69761 AND charttime < (SELECT dischtime FROM admissions WHERE hadm_id = 69761)',\n",
              " '598d1cbbc9c8f37a219ceb3f': \"SELECT * FROM appointments WHERE doctor = 'Dr. Burgess' AND appointment_date = CURRENT_DATE AND appointment_type = 'Outpatient'\",\n",
              " '6bbdda4d7a437ac52e5bad9a': \"SELECT * FROM chartevents WHERE subject_id = 20066 AND itemid = (SELECT itemid FROM d_items WHERE label = 'PETCT examination')\",\n",
              " 'd1804a67e2b6c1925531f4ed': \"SELECT DISTINCT drug FROM prescriptions JOIN admissions ON prescriptions.hadm_id = admissions.hadm_id JOIN diagnoses_icd ON admissions.hadm_id = diagnoses_icd.hadm_id WHERE diagnoses_icd.icd9_code = '276.1'\",\n",
              " 'a5f974c50be491ae4b136407': \"SELECT enddate - startdate AS duration FROM prescriptions WHERE subject_id = 10855 AND drug = 'entral infus nutrit sub'\",\n",
              " '50fe1ef26379429b9064604f': \"SELECT * FROM appointments WHERE doctor = 'Dr. Dunlow' AND appointment_date = CURRENT_DATE AND appointment_type = 'Outpatient'\",\n",
              " '2b0e79d8db1967b544b97565': 'SELECT drug FROM prescriptions WHERE subject_id = 54390\"',\n",
              " '773ac05da68fd87a3ac2c808': \"SELECT label FROM d_items WHERE linksto = 'transfusions' AND label LIKE '%precautions%'\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.data_io import write_json as write_label\n",
        "\n",
        "# Save the filtered predictions to a JSON file\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "SCORING_OUTPUT_DIR = os.path.join(RESULT_DIR, 'prediction.json')\n",
        "write_label(SCORING_OUTPUT_DIR, label_y)\n",
        "\n",
        "# Verify the file creation\n",
        "print(\"Listing files in RESULT_DIR:\")\n",
        "!ls {RESULT_DIR}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUMTiYl4hdt_",
        "outputId": "f45ecba9-0051-4605-b302-529bfbd41b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing files in RESULT_DIR:\n",
            "prediction.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Submission\n",
        "\n",
        "In this final step, we'll prepare and submit our results to the Codabench competition."
      ],
      "metadata": {
        "id": "nJT6jj7WiAkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the directory containing the prediction file\n",
        "%cd {RESULT_DIR}\n",
        "\n",
        "# Compress the prediction.json file into a ZIP archive\n",
        "!zip predictions.zip prediction.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A9_Oi0AhfQU",
        "outputId": "d2797229-cefd-4439-ce6e-257ce6bd6e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehrsql-2024/sample_result_submission\n",
            "  adding: prediction.json (deflated 65%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Submission File: Ensure that the `predictions.zip` file contains only the `prediction.json` file. This ZIP archive is the required format for submission to Codabench.\n",
        "\n",
        "- Submitting on Codabench: Navigate to the Codabench competition page and go to the **My Submissions** tab. Upload the `predictions.zip` file following the provided instructions. Make sure to adhere to any guidelines or submission requirements detailed on the competition page."
      ],
      "metadata": {
        "id": "a-LDPEnlh8wN"
      }
    }
  ]
}